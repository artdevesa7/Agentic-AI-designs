{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fb52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a641e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eee5a3df5b74a13ad46b84a72507b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artur\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\artur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d23d9fe525846939288ab2a70d2d6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcadb79273244999684f3a482d9f748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f442b0f242e4d76bf6d22b2b7ae5a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6599f671c4d3691633a8c1fb6a78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# Example input text\n",
    "\n",
    "text = \"Hello, how are you doing today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0cdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text and add special tokens\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baebca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d62460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0050, -0.0445, -0.2543,  ..., -0.4674,  0.1839,  0.4307],\n",
      "        [ 0.1518, -0.3346,  0.0500,  ..., -0.0397,  0.9728,  0.3340],\n",
      "        [-0.5165,  0.2028,  0.5212,  ..., -0.5615,  0.4370,  0.1435],\n",
      "        ...,\n",
      "        [-0.2533, -0.7244, -0.7809,  ..., -0.1901, -0.0070, -0.6356],\n",
      "        [-0.3863, -0.8991, -0.8409,  ...,  0.0749,  0.1376, -0.1122],\n",
      "        [ 0.5416, -0.1070, -0.3464,  ...,  0.1287, -0.4294, -0.2204]])\n"
     ]
    }
   ],
   "source": [
    "# Print embeddings\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "write code in python to create embeddings using bert\n",
    "\n",
    "\n",
    "To create embeddings using BERT in Python, you can use the transformers library, which provides pre-trained BERT models and an easy-to-use interface for generating embeddings. Here's an example code snippet:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input text\n",
    "text = \"Hello, how are you doing today?\"\n",
    "\n",
    "# Tokenize input text and add special tokens\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0][0]\n",
    "\n",
    "# Print embeddings\n",
    "print(embeddings)\n",
    "In this example, we first load the pre-trained BERT tokenizer and model using the AutoTokenizer and AutoModel classes from the transformers library. Then, we define an example input text and tokenize it using the tokenizer, adding special tokens for BERT. Next, we generate BERT embeddings for the tokenized input text by passing it through the model and extracting the output embeddings. Finally, we print the embeddings.\n",
    "\n",
    "Note that the above code generates embeddings for a single input text. To generate embeddings for multiple input texts, you can batch the input token IDs and pass them through the BERT model all at once, like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Example input texts\n",
    "texts = [\"Hello, how are you doing today?\", \"I am doing well, thank you.\"]\n",
    "\n",
    "# Tokenize input texts and add special tokens\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True) for text in texts])\n",
    "\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0]\n",
    "\n",
    "# Print embeddings\n",
    "print(embeddings)\n",
    "In this example, we define a list of example input texts and tokenize each one, adding special tokens as before. Then, we batch the input token IDs and pass them through the BERT model all at once, generating embeddings for each input text. Finally, we print the embeddings for all input texts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Arturo Devesa\n",
    "which part of the code is the batching?\n",
    "In the second code snippet, the batching is done in the following lines:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Example input texts\n",
    "texts = [\"Hello, how are you doing today?\", \"I am doing well, thank you.\"]\n",
    "\n",
    "# Tokenize input texts and add special tokens\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True) for text in texts])\n",
    "\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0]\n",
    "Here, we first define a list of example input texts texts. Then, we tokenize each input text using the BERT tokenizer and add special tokens as before. This is done using a list comprehension:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True) for text in texts])\n",
    "The resulting input_ids tensor will have shape (num_texts, max_seq_length), where num_texts is the number of input texts and max_seq_length is the maximum sequence length of the tokenized inputs.\n",
    "\n",
    "Finally, we pass the input_ids tensor through the BERT model all at once to generate embeddings for all input texts in a single forward pass:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0]\n",
    "The resulting embeddings tensor will have shape (num_texts, max_seq_length, hidden_size), where hidden_size is the size of the BERT model's hidden layer. Each row of the embeddings tensor corresponds to the embeddings for one input text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Arturo Devesa\n",
    "I don't understand the with torch.no_grad(): part of the code\n",
    "The with torch.no_grad(): statement is used in PyTorch to temporarily disable gradient computation during a forward pass through the model. This is useful in situations where you don't need to compute gradients, such as during inference or evaluation, because it can save memory and speed up computation.\n",
    "\n",
    "In the context of generating embeddings using BERT, we use the with torch.no_grad(): statement to disable gradient computation during the forward pass through the BERT model. This is because we don't need to compute gradients when generating embeddings, and it can be computationally expensive to do so.\n",
    "\n",
    "Here's how it works in the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    outputs = model(input_ids)  # Forward pass through BERT model\n",
    "    embeddings = outputs[0]  # Extract output embeddings\n",
    "\n",
    "# Print embeddings\n",
    "print(embeddings)\n",
    "Inside the with torch.no_grad(): block, we perform a forward pass through the BERT model using the model(input_ids) statement. Since we have disabled gradient computation, no gradients are computed during this operation. Instead, the model simply computes the embeddings for the input texts using the pre-trained weights of the BERT model.\n",
    "\n",
    "Once we have the output embeddings, we extract them using the outputs[0] statement and store them in the embeddings variable. Then, we print the embeddings to the console.\n",
    "\n",
    "So, in summary, the with torch.no_grad(): statement is used to disable gradient computation during the forward pass through the BERT model, which can save memory and speed up computation during inference or evaluation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Arturo Devesa\n",
    "sounds good, thanks. and why do we call outputs[0] at index 0?\n",
    "In PyTorch, the outputs variable returned by a model's forward method is a tuple containing various outputs of the model. The specific contents of the tuple depend on the model and its configuration.\n",
    "\n",
    "In the case of the BERT model, the outputs tuple contains two elements:\n",
    "\n",
    "outputs[0]: The last-layer hidden states of the model, i.e., the output embeddings.\n",
    "outputs[1]: The pooled output of the model, i.e., a summary representation of the input sequence that can be used for classification tasks.\n",
    "Since we want to extract the output embeddings from the BERT model, we access the outputs tuple at index 0, which corresponds to the last-layer hidden states. This returns a tensor containing the output embeddings for the input text(s).\n",
    "\n",
    "So, the line embeddings = outputs[0] extracts the output embeddings from the outputs tuple and assigns them to the embeddings variable, which we then print to the console.\n",
    "\n",
    "Note that the shape of embeddings is (num_tokens, hidden_size), where num_tokens is the number of tokens in the input sequence(s) and hidden_size is the size of the BERT model's hidden layer. Each row of the embeddings tensor corresponds to the embeddings for one token in the input sequence(s).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Arturo Devesa\n",
    "ah ok, I didn't know that, thanks. And why do we call the variable input_ids with the word ids? I understand why use the word input, but why do we use the word ids in the variable name?\n",
    "In NLP, when we represent text using numerical values, we often use IDs (short for \"identifiers\") to refer to individual words or tokens. These IDs are typically integers that correspond to the index of a word in a vocabulary, where the most common words have lower IDs and less common words have higher IDs.\n",
    "\n",
    "In the case of BERT, the tokenizer.encode method returns a list of word IDs that correspond to the tokenized input text(s). These IDs are used to represent the input text(s) numerically, which can then be passed to the BERT model for computation.\n",
    "\n",
    "So, in the code, we use the variable name input_ids to refer to the tensor of word IDs that represent the input text(s). This is a common convention in NLP, where we often use the suffix \"ids\" to refer to numerical representations of text using IDs.\n",
    "\n",
    "In summary, we use the variable name input_ids to refer to the tensor of word IDs that represent the input text(s) numerically using IDs. This is a common convention in NLP, where we often use the suffix \"ids\" to refer to numerical representations of text using IDs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Arturo Devesa\n",
    "so are input_ids kind of like saying token ids?\n",
    "Yes, that's a good way to think about it. In the context of BERT, the input_ids variable represents the token IDs of the input text(s). More specifically, it is a tensor of shape (batch_size, sequence_length) where each element is an integer that corresponds to the ID of a token in the BERT vocabulary.\n",
    "\n",
    "Note that the input_ids variable is generated by the tokenizer.encode method, which tokenizes the input text(s) and maps the resulting tokens to their corresponding IDs in the vocabulary. So, in essence, input_ids is a numerical representation of the input text(s) that is based on the BERT vocabulary.\n",
    "\n",
    "In summary, input_ids represents the token IDs of the input text(s), and is a numerical representation of the text that is based on the BERT vocabulary.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
